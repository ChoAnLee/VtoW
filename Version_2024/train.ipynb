{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpgxyhlBsflo"
      },
      "source": [
        "# With Wandb pipline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaffGwGKsflr"
      },
      "source": [
        "### setup environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFztSg_dsflr",
        "outputId": "dc0276a9-bc2b-4933-b2c4-a4de3e072e24"
      },
      "outputs": [],
      "source": [
        "%pip install transformers datasets\n",
        "%pip install wandb\n",
        "%pip install torch\n",
        "%pip install torchmetrics\n",
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTBInW5rsflt"
      },
      "source": [
        "### import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNrQkwOTsflt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "import random\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "# %wandb login\n",
        "wandb.login()\n",
        "\n",
        "# Set Ramdom Seed\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip97faB6sflu"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSse3cWZsflu"
      },
      "source": [
        "### Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XevqHQDEsflv",
        "outputId": "6f8abe4e-29ab-4b13-96fe-6f6035c53eb3"
      },
      "outputs": [],
      "source": [
        "# Gobal variables\n",
        "PROGJECT_NAME = 'WIDM-MuIME-LGClassification'\n",
        "DATASET_ROOT_PATH = \".//Dataset//Train_Datasets//\"\n",
        "MODEL_SAVE_PATH = \"./\"\n",
        "TRAIN_FILENAME = \"bopomofo-0.txt\"\n",
        "\n",
        "WANDB_LOG = False\n",
        "INPUT_SEQUENCE_LENGTH = 20\n",
        "SEQUENCE_SIZE = INPUT_SEQUENCE_LENGTH\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "config = dict(\n",
        "    epochs = 10,\n",
        "    learning_rate = 3e-4,\n",
        "    batch_size = 16,\n",
        "    optimizer = \"Adam\",\n",
        "    loss_function = \"MSELoss\",\n",
        "\n",
        "    dataset=TRAIN_FILENAME,\n",
        "    dataset_error_rate=0.0,\n",
        "    # sequence_size = 128,\n",
        "    # embedding_size = 768,\n",
        "    # example_size = 500,\n",
        "    # classes=10,\n",
        "    # kernels=[16, 32],\n",
        "    # dataset=\"MNIST\",\n",
        "    device=device,\n",
        "    architecture = \"Linear Feed Forward\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPyScc12sflv"
      },
      "source": [
        "## Pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xN52SS2AoL1Z"
      },
      "outputs": [],
      "source": [
        "def tensorlog(name:str, x:torch.Tensor) -> None:\n",
        "    print(\"{}: {}\".format(name, x))\n",
        "    print(\"{} shape: {}\".format(name, x.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaSuGVmzsflv"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Moved to MyModel directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XjhGWsGsflu"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8BfBApXsflu"
      },
      "outputs": [],
      "source": [
        "# Moved to data_preprocessing.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxYLKB9Tsflv"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp5hdqfjsflw"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MyDataSet(Dataset):\n",
        "    def __init__(self, X, Y, transform=None):\n",
        "        self.data = X\n",
        "        self.labels = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEOuyIdcsflw"
      },
      "source": [
        "### train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBYoJfD7sflw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    print(\"start training...\")\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=10) if WANDB_LOG else None\n",
        "\n",
        "\n",
        "    model.to(config[\"device\"])\n",
        "    \n",
        "    total_batches = len(train_loader) * config.epochs\n",
        "    total_batch = len(train_loader)\n",
        "    example_ct = 0  # number of examples seen\n",
        "\n",
        "    best_val_acc = 0\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "\n",
        "        # ==================== Train =====================\n",
        "        model.train()\n",
        "        train_loss, train_acc = 0.0, 0.0\n",
        "        for t_batch, (inputs, labels) in enumerate(train_loader):\n",
        "            batch_loss, correct_batch_token, total_batch_token = train_batch(inputs, labels, model, optimizer, criterion)\n",
        "            batch_acc = correct_batch_token / total_batch_token\n",
        "            example_ct +=  len(inputs)\n",
        "            train_loss += batch_loss\n",
        "\n",
        "            train_log(epoch, t_batch, total_batch, batch_loss, batch_acc, example_ct)\n",
        "\n",
        "\n",
        "        # calculate train_acc\n",
        "        print('\\nTrain | Loss: {:.5f} Acc: {:.3f}%'.format(train_loss, train_acc))\n",
        "\n",
        "\n",
        "\n",
        "        # ==================== Valid =====================\n",
        "        model.eval()\n",
        "        val_loss, val_acc = 0, -404\n",
        "        total_correct_token, total_token = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for t_val_batch, (inputs, labels) in enumerate(val_loader):\n",
        "                val_batch_loss, correct_batch_token, total_batch_token = val_batch(inputs, labels, model, criterion)\n",
        "                val_batch_acc = correct_batch_token / total_batch_token\n",
        "\n",
        "                val_loss += val_batch_loss\n",
        "                total_correct_token += correct_batch_token\n",
        "                total_token += total_batch_token\n",
        "\n",
        "        val_acc = total_correct_token / total_token\n",
        "        val_log(epoch, val_loss, val_acc, example_ct)\n",
        "\n",
        "\n",
        "        # todo: if val > train safe model\n",
        "        # save with wandb.save\n",
        "        if val_acc > best_val_acc:\n",
        "            print(\"Saving model with acc {}\".format(val_acc))\n",
        "            torch.save(model.state_dict(), MODEL_SAVE_PATH + '/model.pth')\n",
        "\n",
        "\n",
        "def train_batch(inputs, labels, model, optimizer, criterion):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(inputs, labels) # (N, S, C)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # matrix = torch.eye(21128).to(device)\n",
        "    # embed_labels = matrix[labels].permute(0, 2, 1).to(device) # (N, S) -> (N, C, S)\n",
        "    loss = criterion(outputs.permute(0, 2, 1), labels)\n",
        "\n",
        "\n",
        "    reduced_outputs = torch.argmax(outputs, dim=-1) # (N, S)\n",
        "    print(\"reduced_outputs: \", reduced_outputs)\n",
        "    print(\"labels: \", labels)\n",
        "    acc = (reduced_outputs.flatten(0) == labels.flatten(0)).sum() / labels.numel()\n",
        "\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # avoid exploding gradient\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item(), (reduced_outputs.flatten(0) == labels.flatten(0)).sum(), labels.numel()\n",
        "\n",
        "\n",
        "def val_batch(inputs, labels, model, criterion):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(inputs, labels)\n",
        "    \n",
        "    # .permute(0, 2, 1)  # (N, S, C) -> (N, C, S)\n",
        "    # matrix = torch.eye(21128).to(device)\n",
        "    # embed_labels = matrix[labels].permute(0, 2, 1)  # (N, S) -> (N, C, S)\n",
        "    loss = criterion(outputs.permute(0, 2, 1), labels)\n",
        "\n",
        "    reduced_outputs = torch.argmax(outputs, dim=-1) # (N, S)\n",
        "    acc = (reduced_outputs.flatten(0) == labels.flatten(0)).sum() / labels.numel()\n",
        "\n",
        "\n",
        "    return loss.item(), (reduced_outputs.flatten(0) == labels.flatten(0)).sum(), labels.numel()\n",
        "\n",
        "def train_log(epoch:int, t_batch:int, total_batch:int, batch_loss:float, batch_acc:float, example_ct:int):\n",
        "    wandb.log({\"train_loss\": batch_loss, \"train_acc\": batch_acc, \"Epoach\": epoch+1}, step=example_ct) if WANDB_LOG else None\n",
        "    print(\"\\r[ Epoch {}: {}/{} ] Batch loss: {:.3f} Batch acc: {:.3f}%\".format(epoch+1, t_batch+1, total_batch, batch_loss, batch_acc*100), end=\"\")\n",
        "\n",
        "\n",
        "def val_log(epoch:int, val_loss:int, val_acc:int, example_ct:int):\n",
        "    wandb.log({\"val_loss\": val_loss, \"val_acc\": val_acc}, step=example_ct) if WANDB_LOG else None\n",
        "    print(\"Valid | Loss: {:.5f} Acc: {:.3f}% \".format(val_loss, val_acc*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loqUnwKysflw"
      },
      "source": [
        "#### Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmFlWFpvsflw",
        "outputId": "08e61972-ab06-4985-d6f3-9b6304c9f97c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from MyTrainLib.KeystrokeTokenizer import KeystrokeTokenizer\n",
        "\n",
        "example_size = 100\n",
        "\n",
        "def load_training_dataset(dataset_path):\n",
        "    print(\"loading datasets: {}...\".format(dataset_path))\n",
        "\n",
        "    X, Y = [], []\n",
        "    with open(dataset_path, \"r\", encoding=\"utf-8\") as dataset:\n",
        "        lines = dataset.readlines()\n",
        "\n",
        "\n",
        "    def pad(token_id_list:list[int]) -> list[int]:\n",
        "        if len(token_id_list) > SEQUENCE_SIZE:\n",
        "            token_id_list = token_id_list[:SEQUENCE_SIZE]\n",
        "        else:\n",
        "            pad_len = SEQUENCE_SIZE - len(token_id_list)\n",
        "            token_id_list += [0] * pad_len\n",
        "        return token_id_list\n",
        "\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        x_str, y_str = line.strip().split(\"\\t\")\n",
        "\n",
        "        x_tokens = KeystrokeTokenizer.tokenize(x_str)\n",
        "        x_ids = KeystrokeTokenizer.token_to_ids(x_tokens)\n",
        "        y_ids = int(y_str)\n",
        "        x_ids = pad(x_ids)\n",
        "        x_ids = torch.tensor(x_ids)\n",
        "        y_ids = torch.tensor(y_ids)\n",
        "\n",
        "        X.append(x_ids)\n",
        "        Y.append(y_ids)\n",
        "        if i > example_size:\n",
        "            break\n",
        "\n",
        "    print(\"Full Datasets:\")\n",
        "    print(\"X:\", len(X), \"Y:\", len(Y))\n",
        "    return MyDataSet(X, Y)\n",
        "\n",
        "# load_training_dataset(DATASET_ROOT_PATH + TRAIN_FILENAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyORTxZMsflw"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "\n",
        "def get_dataset(slice_rate=0.8):\n",
        "    print(\"preparing datasets...\")\n",
        "    full_dataset = load_training_dataset(DATASET_ROOT_PATH + TRAIN_FILENAME)\n",
        "\n",
        "    at = int(len(full_dataset) * slice_rate)\n",
        "    train_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, at))\n",
        "    val_dataset = torch.utils.data.Subset(full_dataset, indices=range(at,))\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def make_loader(dataset, batch_size):\n",
        "    NUM_OF_WORKERS = 1 if platform.system() == \"Windows\" else 2\n",
        "    print(\"running on system: {}\".format(platform.system()))\n",
        "    print(\"Setting num of workers: {}\".format(NUM_OF_WORKERS))\n",
        "\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=True,\n",
        "                                         pin_memory=True,\n",
        "                                         num_workers=NUM_OF_WORKERS,\n",
        "                                         drop_last=True)\n",
        "    return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYhIhHA4sflx"
      },
      "source": [
        "#### Make\n",
        "\n",
        "Define model, criterion, and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XS7H5xksflx"
      },
      "outputs": [],
      "source": [
        "from MyTrainLib.LCModel import LCModel\n",
        "\n",
        "def make(config):\n",
        "    # Make the data\n",
        "    train, val = get_dataset()\n",
        "    print(\"Training set size: {}, Validation set size: {}\".format(train.__len__(), val.__len__()))\n",
        "\n",
        "    print(\"batch size:\", config.batch_size)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    val_loader = make_loader(val, batch_size=config.batch_size)\n",
        "\n",
        "    # Model\n",
        "    model = LCModel(layers=[20, 40, 1])\n",
        "\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = nn.MSELoss() \n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=config.learning_rate)\n",
        "    # todo: lr sheulare\n",
        "\n",
        "    return model, train_loader, val_loader, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzRuMmsTYAuY"
      },
      "outputs": [],
      "source": [
        "def log_model_info(model) -> None:\n",
        "    print(model)\n",
        "    total_parameters = sum(p.numel() for p in model.parameters())\n",
        "    trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total: {}, trainable: {}\\n'.format(total_parameters, trainable_parameters))\n",
        "    wandb.config.update({\"Total parameter\": total_parameters, \"Trainable parameters:\": trainable_parameters}) if WANDB_LOG else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WSHb74Usflx"
      },
      "source": [
        "#### Pipline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJe9_5xdsflx"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "    \n",
        "    WANDB_LOG = True\n",
        "    if WANDB_LOG:\n",
        "      run =  wandb.init(project=PROGJECT_NAME, config=hyperparameters) # todo: as run\n",
        "      config = wandb.config\n",
        "    else:\n",
        "      class ConfigWrapper:  # fixme: ugly\n",
        "        def __init__(self, config_dict):\n",
        "            self.__dict__ = config_dict\n",
        "\n",
        "      config = ConfigWrapper(hyperparameters) \n",
        "    \n",
        "    # make the model, data, and optimization\n",
        "    model, train_loader, val_loader, criterion, optimizer = make(config)\n",
        "    log_model_info(model)\n",
        "\n",
        "    # Training\n",
        "    train(model, train_loader, val_loader, criterion, optimizer, config)\n",
        "\n",
        "    # Testing\n",
        "    # test(model, test_loader)\n",
        "       \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_i4omUNsflx"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5127a4253bd24bc8a731ab386c498427",
            "1e65c6ee83a24342991c0b3938d7c485",
            "38ed803fff3e4025a1f753116be82c47",
            "244e5401b34741a69e911b722434d83c",
            "c7568344822747a8940ef658f52d9215",
            "244f4d537579413a9a9faad5c96eb7f9",
            "e889cd167db643aba5d0b2c2045d3491",
            "1ed8d05865b848f38bf11f832d7133b6",
            "f6bce43e449342ae9a62959ba86f4fa1",
            "7a7080627a15423995dcb22ac9a31af2",
            "05d6437802aa4338a44ed5cd6d734b06"
          ]
        },
        "id": "fdm4OmH9sflx",
        "outputId": "a69f40a4-6f8b-4692-9f1a-8a0de20df6e3"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "model = model_pipeline(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgu00z4nszZm"
      },
      "source": [
        "#### TEST AREA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uwpdvc9JVwi"
      },
      "outputs": [],
      "source": [
        "# from torchmetrics.classification import MultilabelAccuracy, MulticlassAccuracy\n",
        "# from torch import tensor\n",
        "# import numpy as np\n",
        "\n",
        "# target = tensor([[[1, 0], [0, 1], [0, 0]], [[1, 0], [1, 0], [1, 0]], [[1, 0], [1, 0], [1, 0]], [[1, 0], [1, 0], [1, 0]]])\n",
        "# preds = tensor(\n",
        "#      [\n",
        "#          [[1, 3], [1, 0], [1, 1]],\n",
        "#          [[0, 0.04], [0.86, 0.780], [0.45, 0.37]],\n",
        "#          [[0, 0.04], [0.86, 0.780], [0.45, 0.37]],\n",
        "#          [[0, 1], [1, 1], [1, 1]],\n",
        "#      ])\n",
        "\n",
        "# target = tensor([[1, 1], [2, 1]])\n",
        "\n",
        "# preds = tensor([[[0., 0.], [10., 0.], [0., 0.]],\n",
        "#                 [[0., 10.], [0., 200.], [10., 0.]],\n",
        "#                 ])\n",
        "\n",
        "# print(\"target:\", target.numel())\n",
        "# print(\"preds:\", preds.shape)\n",
        "\n",
        "# reduced_prediction_tensor = np.argmax(preds, axis=1)\n",
        "# print(reduced_prediction_tensor)\n",
        "# accuracy = len()/(reduced_prediction_tensor.flatten(0) == target).sum()\n",
        "\n",
        "\n",
        "# print(accuracy)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05d6437802aa4338a44ed5cd6d734b06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e65c6ee83a24342991c0b3938d7c485": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_244f4d537579413a9a9faad5c96eb7f9",
            "placeholder": "​",
            "style": "IPY_MODEL_e889cd167db643aba5d0b2c2045d3491",
            "value": "  0%"
          }
        },
        "1ed8d05865b848f38bf11f832d7133b6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "244e5401b34741a69e911b722434d83c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a7080627a15423995dcb22ac9a31af2",
            "placeholder": "​",
            "style": "IPY_MODEL_05d6437802aa4338a44ed5cd6d734b06",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "244f4d537579413a9a9faad5c96eb7f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ed803fff3e4025a1f753116be82c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ed8d05865b848f38bf11f832d7133b6",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6bce43e449342ae9a62959ba86f4fa1",
            "value": 0
          }
        },
        "5127a4253bd24bc8a731ab386c498427": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e65c6ee83a24342991c0b3938d7c485",
              "IPY_MODEL_38ed803fff3e4025a1f753116be82c47",
              "IPY_MODEL_244e5401b34741a69e911b722434d83c"
            ],
            "layout": "IPY_MODEL_c7568344822747a8940ef658f52d9215"
          }
        },
        "7a7080627a15423995dcb22ac9a31af2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7568344822747a8940ef658f52d9215": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e889cd167db643aba5d0b2c2045d3491": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6bce43e449342ae9a62959ba86f4fa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
