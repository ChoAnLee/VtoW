{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Wandb pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.33.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.17.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (1.5.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->datasets) (2022.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.15.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.21.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: pathtools in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (57.4.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (4.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (4.22.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jason\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets\n",
    "%pip install wandb\n",
    "%pip install torch\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src='https://wandb.ai/falcon-team/login/workspace?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x15e112ddf70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjasonjasonyehyeh\u001b[0m (\u001b[33mfalcon-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "%wandb login\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from CangjieKeyMap import CangjieKeyMap\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def cangjie_mixer():\n",
    "    CHINESE_NEWS_TXT_PATH = \".\\dataset\\\\chinese_news.txt\"\n",
    "    CHINESE_NEWS_CANGJIE_MIX_PATH = \".\\dataset\\chinese_news_cangjie_mix.txt\"\n",
    "\n",
    "    cangjie_key_map = CangjieKeyMap()\n",
    "\n",
    "    with open(CHINESE_NEWS_TXT_PATH, \"r\", encoding=\"utf-8\") as news_file:\n",
    "        with open(CHINESE_NEWS_CANGJIE_MIX_PATH, \"w\", encoding=\"utf-8\") as news_mix_file:\n",
    "            for line in news_file:\n",
    "                line = line.strip()\n",
    "                cangjie_line = cangjie_key_map.convert_to_cangjie_key(line)\n",
    "                news_mix_file.write(cangjie_line + \"\\t\" + line + \"\\n\")\n",
    "\n",
    "# cangjie_mixer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Gobal variables\n",
    "PROGJECT_NAME = 'WIDM-MuIME'\n",
    "DATASET_ROOT_PATH = \"./datasets\"\n",
    "MODEL_SAVE_PATH = \"./models\"\n",
    "CHINESE_NEWS_CANGJIE_MIX_DATASETS_PATH = \"./datasets/chinese_news_cangjie_mix.txt\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs = 10,\n",
    "    learning_rate = 0.001,\n",
    "    batch_size = 16,\n",
    "    optimizer = \"Adam\",\n",
    "    # classes=10,\n",
    "    # kernels=[16, 32],\n",
    "    # batch_size=128,\n",
    "    # learning_rate=0.005,\n",
    "    # dataset=\"MNIST\",\n",
    "    architecture = \"Transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Optional, Union\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class Cangjie2ChineseTransformer(nn.Transformer):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, X, Y, transform=None):\n",
    "        self.data = X\n",
    "        self.labels = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
    "    print(\"start training...\")\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(train_loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0.0, 0.0\n",
    "        for t_batch, (inputs, labels) in enumerate(train_loader):\n",
    "            batch_loss = train_batch(inputs, labels, model, optimizer, criterion)\n",
    "            batch_acc = -404 # todo: calculate acc\n",
    "            example_ct +=  len(inputs)\n",
    "            batch_ct += 1\n",
    "\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_ct + 1) % 25) == 0:\n",
    "                train_log(batch_loss, example_ct, epoch)\n",
    "    \n",
    "            print(\"\\r[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f}%\".format(epoch+1, i+1, t_batch, batch_loss.item(), \"-404\"), end=\"\")\n",
    "        \n",
    "\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}%'.format(train_loss, train_acc))\n",
    "        # ==================== valid =====================\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(val_loader):\n",
    "                val_loss = val_batch(inputs, labels, model, criterion)\n",
    "\n",
    "        # todo: if val > train safe model\n",
    "        # save with wandb.save\n",
    "\n",
    "\n",
    "def train_batch(inputs, labels, model, optimizer, criterion):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    # Forward pass ➡\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Backward pass ⬅\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Step with optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")\n",
    "\n",
    "\n",
    "def val_batch(inputs, labels, model, criterion):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def load_training_dataset():\n",
    "    dataset_path = CHINESE_NEWS_CANGJIE_MIX_DATASETS_PATH\n",
    "\n",
    "    X, Y = [], []\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as dataset:\n",
    "        lines = dataset.readlines()\n",
    "\n",
    "    def one_hot_encode(x:str) -> torch.Tensor: # 可優化? \n",
    "        key_labels = [\"`\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\", \"-\", \"=\", \"q\", \"w\", \"e\", \"r\", \"t\", \"y\", \"u\", \"i\", \"o\", \"p\", \"[\", \"]\", \"\\\\\",\n",
    "                      \"a\", \"s\", \"d\", \"f\", \"g\", \"h\", \"j\", \"k\", \"l\", \";\", \"'\", \"shift\", \"z\", \"x\", \"c\", \"v\", \"b\", \"n\", \"m\", \",\", \".\", \"/\", \"ctrl\", \" \"\n",
    "        ]\n",
    "        one_hot_matrix = torch.eye(len(key_labels))\n",
    "        \n",
    "        tensor_list = []\n",
    "        for char in x:\n",
    "            if char in key_labels:\n",
    "                tensor_list.append(list(one_hot_matrix[key_labels.index(char)]))\n",
    "            else:\n",
    "                tensor_list.append([0] * len(key_labels))\n",
    "            \n",
    "        return torch.tensor(tensor_list)\n",
    "\n",
    "    def chinese_word_encode(x:str) -> torch.Tensor: # todo\n",
    "        return torch.zeros(len(x), 50)\n",
    "\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        x_str, y_str = line.strip().split(\"\\t\")\n",
    "        x_tensor = one_hot_encode(x_str)\n",
    "        y_tensor = chinese_word_encode(y_str)\n",
    "        X.append(x_tensor)\n",
    "        Y.append(y_tensor)\n",
    "        print(x_tensor.shape, y_tensor.shape)\n",
    "        if i > 10:\n",
    "            break\n",
    "    \n",
    "    print(\"X:\", len(X), \"Y:\", len(Y))\n",
    "    return MyDataSet(X, Y)\n",
    "\n",
    "# load_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "def get_dataset(slice_rate=0.8):\n",
    "    print(\"preparing datasets\")\n",
    "    full_dataset = load_training_dataset()\n",
    "    \n",
    "    at = int(len(full_dataset) * slice_rate)\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, at))\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, indices=range(at,))\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    NUM_OF_WORKERS = 1 if platform.system() == \"Windows\" else 2\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         pin_memory=True, num_workers=NUM_OF_WORKERS)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make\n",
    "\n",
    "Define model, criterion, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make(config):\n",
    "    # Make the data\n",
    "    train, val = get_dataset()\n",
    "    print(train.__len__(), val.__len__())\n",
    "\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    val_loader = make_loader(val, batch_size=config.batch_size)\n",
    "\n",
    "    # Make the model\n",
    "    # model = ConvNet(config.kernels, config.classes).to(device)\n",
    "    # model = Cangjie2ChineseTransformer()\n",
    "    model = model = AutoModelForMaskedLM.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    return model, train_loader, val_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=PROGJECT_NAME, config=hyperparameters):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, val_loader, criterion, optimizer = make(config)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader, val_loader, criterion, optimizer, config)\n",
    "\n",
    "      # and test its final performance\n",
    "    #   test(model, test_loader)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jason\\Desktop\\Project_Codes\\Course\\special_topic\\translanguaging-IME\\wandb\\run-20230921_184858-njz7ltsv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/falcon-team/WIDM-MuIME/runs/njz7ltsv' target=\"_blank\">fragrant-wave-21</a></strong> to <a href='https://wandb.ai/falcon-team/WIDM-MuIME' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/falcon-team/WIDM-MuIME' target=\"_blank\">https://wandb.ai/falcon-team/WIDM-MuIME</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/falcon-team/WIDM-MuIME/runs/njz7ltsv' target=\"_blank\">https://wandb.ai/falcon-team/WIDM-MuIME/runs/njz7ltsv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing datasets\n",
      "torch.Size([118, 50]) torch.Size([25, 50])\n",
      "torch.Size([64, 50]) torch.Size([16, 50])\n",
      "torch.Size([109, 50]) torch.Size([25, 50])\n",
      "torch.Size([67, 50]) torch.Size([17, 50])\n",
      "torch.Size([91, 50]) torch.Size([22, 50])\n",
      "torch.Size([114, 50]) torch.Size([22, 50])\n",
      "torch.Size([147, 50]) torch.Size([32, 50])\n",
      "torch.Size([138, 50]) torch.Size([32, 50])\n",
      "torch.Size([84, 50]) torch.Size([25, 50])\n",
      "torch.Size([108, 50]) torch.Size([25, 50])\n",
      "torch.Size([135, 50]) torch.Size([30, 50])\n",
      "torch.Size([116, 50]) torch.Size([26, 50])\n",
      "X: 12 Y: 12\n",
      "9 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "start training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e4173c07e4dadbc5c830d2bba5097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-wave-21</strong> at: <a href='https://wandb.ai/falcon-team/WIDM-MuIME/runs/njz7ltsv' target=\"_blank\">https://wandb.ai/falcon-team/WIDM-MuIME/runs/njz7ltsv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230921_184858-njz7ltsv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 10476) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1134\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\queue.py:179\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_empty\u001b[39m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Course\\special_topic\\translanguaging-IME\\train.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_pipeline(config)\n",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Course\\special_topic\\translanguaging-IME\\train.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m   \u001b[39mprint\u001b[39m(model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39m# and use them to train the model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   train(model, train_loader, val_loader, criterion, optimizer, config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   \u001b[39m# and test its final performance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#   test(model, test_loader)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Course\\special_topic\\translanguaging-IME\\train.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m train_loss, train_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m, \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m t_batch, (inputs, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     batch_loss \u001b[39m=\u001b[39m train_batch(inputs, labels, model, optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Course/special_topic/translanguaging-IME/train.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     batch_acc \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m404\u001b[39m \u001b[39m# todo: calculate acc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m   1284\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[1;32m-> 1285\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1286\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1287\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\jason\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 10476) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### todos\n",
    "\n",
    "- [ ] use pip install -r requirements.txt / pip freeze\n",
    "- [ ] set val > train save model\n",
    "- [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  3193,   677,  1962,  8024,   167,  8178,  8152,  8181,   132,\n",
      "           125, 10753,  8152,  8024,  6078,  6079,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# from transformers import AutoModel\n",
    "\n",
    "# checkpoint = \"bert-base-chinese\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# sequence  = \"早上好，yl3g;4cl3，蝴蝶\"\n",
    "\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# print(tokens)\n",
    "# print(inputs.input_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:  13%|█▎        | 52.4M/412M [00:13<01:32, 3.90MB/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\product-version\\train.ipynb Cell 29\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Side%20Project/Multi-Lingual-IME/product-version/train.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoModel\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Side%20Project/Multi-Lingual-IME/product-version/train.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbert-base-chinese\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Side%20Project/Multi-Lingual-IME/product-version/train.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(checkpoint)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jason/Desktop/Project_Codes/Side%20Project/Multi-Lingual-IME/product-version/train.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mconfig)\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39mmodel_args, config\u001b[39m=\u001b[39mconfig, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mhub_kwargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\transformers\\modeling_utils.py:2785\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2770\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   2771\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   2772\u001b[0m     cached_file_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m   2773\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcache_dir\u001b[39m\u001b[39m\"\u001b[39m: cache_dir,\n\u001b[0;32m   2774\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mforce_download\u001b[39m\u001b[39m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2783\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m: commit_hash,\n\u001b[0;32m   2784\u001b[0m     }\n\u001b[1;32m-> 2785\u001b[0m     resolved_archive_file \u001b[39m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   2787\u001b[0m     \u001b[39m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   2788\u001b[0m     \u001b[39m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   2789\u001b[0m     \u001b[39mif\u001b[39;00m resolved_archive_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m filename \u001b[39m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   2790\u001b[0m         \u001b[39m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\transformers\\utils\\hub.py:429\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    427\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    428\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    430\u001b[0m         path_or_repo_id,\n\u001b[0;32m    431\u001b[0m         filename,\n\u001b[0;32m    432\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    433\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    434\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    435\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    436\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    437\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    438\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    439\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    440\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[0;32m    441\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    442\u001b[0m     )\n\u001b[0;32m    443\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    445\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    446\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1429\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1426\u001b[0m         \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1429\u001b[0m     http_get(\n\u001b[0;32m   1430\u001b[0m         url_to_download,\n\u001b[0;32m   1431\u001b[0m         temp_file,\n\u001b[0;32m   1432\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1433\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[0;32m   1434\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1435\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[0;32m   1436\u001b[0m     )\n\u001b[0;32m   1438\u001b[0m \u001b[39mif\u001b[39;00m local_dir \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1439\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mStoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mblob_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:551\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    541\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    543\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[0;32m    544\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    545\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    549\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[0;32m    550\u001b[0m )\n\u001b[1;32m--> 551\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[0;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    553\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\urllib3\\response.py:940\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    939\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 940\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[0;32m    942\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[0;32m    943\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\urllib3\\response.py:879\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m amt:\n\u001b[0;32m    877\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer\u001b[39m.\u001b[39mget(amt)\n\u001b[1;32m--> 879\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raw_read(amt)\n\u001b[0;32m    881\u001b[0m flush_decoder \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\urllib3\\response.py:814\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    811\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    813\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 814\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m amt \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n\u001b[0;32m    816\u001b[0m         \u001b[39m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    817\u001b[0m         \u001b[39m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    822\u001b[0m         \u001b[39m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m         \u001b[39m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\jason\\Desktop\\Project_Codes\\Side Project\\Multi-Lingual-IME\\.venv\\lib\\site-packages\\urllib3\\response.py:799\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    796\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[0;32m    797\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:462\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[0;32m    461\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[1;32m--> 462\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[0;32m    466\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py:506\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    501\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[0;32m    503\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[0;32m    504\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[1;32m--> 506\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[0;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[0;32m    508\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:  13%|█▎        | 52.4M/412M [00:32<01:32, 3.90MB/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# checkpoint = \"bert-base-chinese\"\n",
    "# model = AutoModel.from_pretrained(checkpoint)\n",
    "# print(model.config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
